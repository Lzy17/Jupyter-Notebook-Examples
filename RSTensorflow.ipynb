{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "outputs": [], "source": ["\nimport tensorflow as tf\nimport array\nimport gzip\nimport random\nfrom tensorflow.keras import Model\nfrom collections import defaultdict\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef parse(path):\n    g = gzip.open(path, 'r')\n    for l in g:\n        yield eval(l)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nuserIDs = {}\nitemIDs = {}\ninteractions = []\n\n# Could adapt to any dataset, this one is from\n# https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home\nfor d in parse(\"goodreads_reviews_comics_graphic.json.gz\"):\n    u = d['user_id']\n    i = d['book_id']\n    r = d['rating']\n    if not u in userIDs: userIDs[u] = len(userIDs)\n    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n    interactions.append((u,i,r))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nlen(interactions)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h1 id=\"Latent-Factor-Model\">Latent Factor Model<a class=\"anchor-link\" href=\"#Latent-Factor-Model\">\u00b6</a></h1>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n# mean rating, just for initialization\nmu = sum([r for _,_,r in interactions]) / len(interactions)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Gradient descent optimizer, experiment with learning rate\noptimizer = tf.keras.optimizers.Adam(0.001)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nclass LatentFactorModel(tf.keras.Model):\n    def __init__(self, mu, K, lamb):\n        super(LatentFactorModel, self).__init__()\n        # Initialize to average\n        self.alpha = tf.Variable(mu)\n        # Initialize to small random values\n        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n        self.lamb = lamb\n\n    # Prediction for a single instance (useful for evaluation)\n    def predict(self, u, i):\n        p = self.alpha + self.betaU[u] + self.betaI[i] +\\\n            tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n        return p\n\n    # Regularizer\n    def reg(self):\n        return self.lamb * tf.reduce_sum(self.betaU**2) +\\\n                           tf.reduce_sum(self.betaI**2) +\\\n                           tf.reduce_sum(self.gammaU**2) +\\\n                           tf.reduce_sum(self.gammaI**2)\n    \n    # Prediction for a sample of instances\n    def predictSample(self, sampleU, sampleI):\n        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n        pred = self.alpha + beta_u + beta_i +\\\n               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n        return pred\n    \n    # Loss\n    def call(self, sampleU, sampleI, sampleR):\n        pred = self.predictSample(sampleU, sampleI)\n        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n        return tf.nn.l2_loss(pred - r) / len(sampleR)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Experiment with number of factors and regularization rate\nmodel = LatentFactorModel(mu, 5, 0.00001)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef trainingStep(interactions):\n    Nsamples = 50000\n    with tf.GradientTape() as tape:\n        sampleU, sampleI, sampleR = [], [], []\n        for _ in range(Nsamples):\n            u,i,r = random.choice(interactions)\n            sampleU.append(userIDs[u])\n            sampleI.append(itemIDs[i])\n            sampleR.append(r)\n\n        loss = model(sampleU,sampleI,sampleR)\n        loss += model.reg()\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients((grad, var) for\n                              (grad, var) in zip(gradients, model.trainable_variables)\n                              if grad is not None)\n    return loss.numpy()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# 10 iterations of gradient descent\nfor i in range(10):\n    obj = trainingStep(interactions)\n    print(\"iteration \" + str(i) + \", objective = \" + str(obj))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Prediction (e.g.):\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ninteractions[0]\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nmodel.predict(userIDs['dc3763cdb9b2cae805882878eebb6a32'], itemIDs['18471619']).numpy()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<h1 id=\"Bayesian-Personalized-Ranking-(One-Class-model)\">Bayesian Personalized Ranking (One-Class model)<a class=\"anchor-link\" href=\"#Bayesian-Personalized-Ranking-(One-Class-model)\">\u00b6</a></h1>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nitemsPerUser = defaultdict(set)\nfor u,i,_ in interactions:\n    itemsPerUser[u].add(i)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nitems = list(itemIDs.keys())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Experiment with learning rate\noptimizer = tf.keras.optimizers.Adam(0.001)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nclass BPRbatch(tf.keras.Model):\n    def __init__(self, K, lamb):\n        super(BPRbatch, self).__init__()\n        # Initialize variables\n        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n        # Regularization coefficient\n        self.lamb = lamb\n\n    # Prediction for a single instance\n    def predict(self, u, i):\n        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n        return p\n\n    # Regularizer\n    def reg(self):\n        return self.lamb * tf.nn.l2_loss(self.betaI) +\\\n                           tf.nn.l2_loss(self.gammaU) +\\\n                           tf.nn.l2_loss(self.gammaI)\n    \n    def score(self, sampleU, sampleI):\n        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n        return x_ui\n\n    def call(self, sampleU, sampleI, sampleJ):\n        x_ui = self.score(sampleU, sampleI)\n        x_uj = self.score(sampleU, sampleJ)\n        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n\nmodel = BPRbatch(5, 0.00001)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndef trainingStep(interactions):\n    Nsamples = 50000\n    with tf.GradientTape() as tape:\n        sampleU, sampleI, sampleJ = [], [], []\n        for _ in range(Nsamples):\n            u,i,_ = random.choice(interactions) # positive sample\n            j = random.choice(items) # negative sample\n            while j in itemsPerUser[u]:\n                j = random.choice(items)\n            sampleU.append(userIDs[u])\n            sampleI.append(itemIDs[i])\n            sampleJ.append(itemIDs[j])\n\n        loss = model(sampleU,sampleI,sampleJ)\n        loss += model.reg()\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients((grad, var) for\n                              (grad, var) in zip(gradients, model.trainable_variables)\n                              if grad is not None)\n    return loss.numpy()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Run training iterations (probably want more than 10...)\nfor i in range(10):\n    obj = trainingStep(interactions)\n    print(\"iteration \" + str(i) + \", objective = \" + str(obj))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# In this case just a score, rather than a prediction of a rating\nmodel.predict(userIDs['dc3763cdb9b2cae805882878eebb6a32'], itemIDs['18471619']).numpy()\n\n"], "execution_count": null, "cell_type": "code"}], "metadata": {}}